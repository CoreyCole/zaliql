\vspace{-.1cm}

\babak{Corey revisit this background in the light of the Lalonde data}
\section{Background: Causality Inference in Statistics}
\label{subsec:causalitystatistics}


% \ignore{
% \dan{I'm inclined to remove this paragraph} \babak{I agree, the example is repeated later on} Causal inference aims to
% establish a casual relationship between one variable called the {\em
%   cause} or {\em treatment}, which has two possible values ($0$ and
% $1$) and another variable, called the {\em effect} or {\em outcome}.
% When the treatment value is $1$, we say that the treatment was
% applied; otherwise, we say that the control treatment was applied.  A
% causal experiment is done with respect to a population or collection
% of {\em units}.  A causal relationship implies that if we intervene by
% changing the treatment, then we should expect to see a change in the
% outcome.  For example (referring to Table~\ref{tab:attlist}) a unit is
% a flight, the treatment is Thunder (a binary attribute) and the
% outcome is DepDelayMinutes (the delay in minutes): we are interested
% in whether thunder causes delay, and how much.}



% \subsection{The Potential Outcome Framework} \label{sec:NRCM}
This section describes the Neyman-Rubin Causal Model (NRCM),
which is the basic causal model in statistics.
% \ignore{
% The basic causal model in statistics is called the Neyman-Rubin Causal
% Model (NRCM). This framework views causal effect as comparisons
% between {\em potential outcomes} defined on the same units. This
% section describes the basic framework.}

\begin{table*}[t] \scriptsize
  \centering
  \begin{tabular}{|c|c|c|c|c|c|} \hline
    Unit & $T$           & $X$          & $Y(1)$                & $Y(0)$        &  $Y(1)-Y(0)$ \\
         & (Treatment)   & (Covariates) & (Treated outcome)     & (Control outcome)     & (Causal  Effect)\\
    \hline
    1   & $T_1$ & $X_1$ &  $Y_1(1)$ & $Y_1(0)$ & $Y_1(1) - Y_1(0)$ \\
    2   & $T_2$ & $X_2$ &  $Y_2(1)$ & $Y_2(0)$ & $Y_2(1) - Y_2(0)$ \\
$\ldots$&       &       &  & & \\
    N   & $T_N$ & $X_N$ &  $Y_N(1)$ & $Y_N(0)$ & $Y_N(1) - Y_N(0)$ \\ \hline
  \end{tabular}
  \caption{The Neyman-Rubin Causal Model (NRCM).}
  \label{fig:causal:inference}
\end{table*}

\vspace{-.3cm}

\paragraph*{Average Treatment Effect (ATE)}
In the NRCM we are given a table $R(T,X,Y(0),Y(1))$ with $N$ rows called {\em units},
indexed by $i=1 \ldots N$; see Table~ \ref{fig:causal:inference}.
The binary attribute $T$ is called {\em treatment assignment}
($T=1$ means the unit was treated; $T=0$ means the unit was subjected to control);
$X$ is a vector of attributes called {\em covariates}, unaffected by treatment;
and the two attributes $Y(0), Y(1)$ represent {\em potential outcomes}:
$Y(1)$ is the outcome of the unit if it is exposed to the treatment and
$Y(0)$ is the outcome when it is exposed to the control.
\corey{I'm personally confused by the following sentence and it's not included in Table 1 - 
is $Z$ an attribute or a subject/row/unit?}
For any attribute $Z$ we write $Z_i$ for the value of the $i$'s unit.
The effect caused by the treatment for the $i$th unit,
simply called the {\em treatment effect} for the $i$th unit, is defined as $Y_i(1)-Y_i(0)$.
The goal of causal analysis is to compute the {\em average treatment effect (ATE)}:


\vspace{-.4cm}
\begin{align}
  \ate = \E[Y(1)-Y(0)] = \E[Y(1)] - \E[Y(0)] \ignore{=\frac{1}{N} \sum_i (Y_i(1)-Y_i(0))}  \label{eq:ate}
\end{align}


Throughout this paper $\E[Z]$ refers to the expected value of the
attribute $Z$ of an individual chosen at random from a large
population.  The population is unavailable to us, instead we have the
database which is typically a random sample of $N$ {\em units} from
that population.  Then $\E[Z]$ is estimated by the empirical expected
value, $\E[Z] = \sum_i Z_i/N$, where $Z_i$ is the attribute of the
$i$'th unit in the database.  In this paper we do not address the
sampling error problem, but we point out that the precision of the
estimator increases with the sample size.  \ignore{Thus, a key goal of the
techniques discussed below is to ensure that expected values are
computed over sufficiently large subsets of the data.}
\vspace{-.15cm}

\begin{example} \em \label{exa3} Lalonde (Cont.)
In the Lalonde dataset, $T$ is true if the subject, $Z$, participated in the
job training program. We are measuring the difference in income between those subjects
who participated in the program and those who did not, or $Y_i(1)-Y_i(0)$
\end{example}

Somewhat surprisingly, the model assumes that {\em both} $Y_i(1)$ and
$Y_i(0)$ are available for each unit $i$.  For example, if the
treatment $T$ is Thunder and the outcome is DepDelayMinutes, then the
assumption is that we have {\em both} values DepDelayMinutes, when
thunder was present and when it was absent.  The inclusion of both
outcomes, factual and counterfactual, in the data model is considered
to be one of the key contributions of the NRCM.
Of course, in reality we have only one of these outcomes for each
unit, \eg, if there was a thunder during that flight then we know
$Y_i(1)$ but not $Y_i(0)$, and in this case we simply write $Y_i$ to
denote $Y_i(T_i)$.  This missing value prevents us from computing
$\ate$ using Eq.(\ref{eq:ate}), and is called the {\em fundamental
  problem of causal inference} \cite{Holland1986}.  Therefore, in
order to compute $\ate$, the statistics literature makes further
assumptions.


\vspace{-0.3cm}
\paragraph*{Randomized Data}
The strongest is the {\em independence assumption}, which states that
the treatment mechanism is independent of the potential outcomes,
i.e., $(Y(1), Y(0)) \bigCI T$. Then, it holds that
$\E[Y(1)]=\E[Y(1)|T=1]$ and similarly $\E[Y(0)]=\E[Y(0)|T=0]$ and we
have:\footnote{An additional assumption is actually needed, called
  \emph{Stable Unit Treatment Value Assumptions (SUTVA)}, which states
  that the outcome on one unit is not affected by the treatment of
  another unit, and the treatment attribute $T$ is binary.  We omit
  some details.}
% \footnote{This framework relies on an assumption that referred to
%   as \emph{Stable Unit Treatment Value Assumptions (SUTVA)}.  This
%   assumption states that: (a) the potential outcome of an individual
%   unit is not affected by the treatment assignment to the other units;
%   \ (b) there is only a single version of a treatment. Assumption (a)
%   ensures that there is no interference between the units and
%   potential outcomes are well-defined.  Assumption (b) is required to
%   make causal inference reasonable. Specifically, If this assumption
%   is violated (e.g., when patients are treated with different dosage
%   of a drug), then inference is not reasonable, because potential
%   outcomes of different units are not comparable.}
%
\vspace{-.1cm}
\begin{eqnarray}
% \nonumber % Remove numbering (before each equation)
  \ate &=& \E[Y(1)|T=1]-\E[Y(0)|T=0] \label{eq:indeass}
\end{eqnarray}
Each expectation above is easily computed from the data, for example
$\E[Y(1)|T=1] = \sum_{i: T_i=1} Y_i / N_1$ where $N_1$ is the number
of units with $T=1$.  The golden standard in causal analysis are {\em
  randomized experiments}, where treatments are assigned randomly to
the units to ensure independence; for example, in medical trials each
subject is randomly assigned the treatment or a placebo, which implies
independence.
\begin{figure*} \scriptsize
  \centering
  \begin{tabular}{|l|l|l|} \hline
    \bf{Name} &  $\delta(x_i,x_j)=$ & \bf{Comments} \\ \hline
Coarsened distance & 0 if $C(x_i)=C(x_j)$ & Where $C(x)$ is a function that coarsen \\
               & $\infty$ if $C(x_i) \neq C(x_j)$ & a vector of continues covariate~\cite{IacKinPor09}\\ \hline
Propensity score  & $|E(x_i) - E(x_j)|$ & where $E(x) = \textrm{Pr}(T = 1 | X=x)$\\
distance (PS)   &  & is the propensity score~\cite{Rubin1983b} \\ \hline
Mahalanobis Distance (MD) & $(x_i-x_j)'\Sigma^{-1} (x_i-x_j)$ & where $\Sigma=$ covariance matrix~\cite{Stuart10} \\ \hline
  \end{tabular}
  \caption{Distance Measures used in Matching}
  \label{fig:metrics}
\end{figure*}
\vspace{-0.3cm}
\paragraph*{Observational Data}
In this paper, however, we are interested in causal analysis in {\em
  observational data}, where the mechanism used to assign treatments
to units is not known, and where independence fails in general.  For
example, thunders occur mostly in the summer, which is also high
travel season and therefore delays may be caused by the high traffic.
In that case $T$ and $Y(0)$ (the delay when thunder does not occur)
are correlated, since they are high in the summer and low in the
winter, and similarly for $T,Y(1)$.  The vast majority of datasets
available to analysts today are observational data, and this motivates
our interest in this case.  Here, the statistics literature makes the
following weaker assumption~\cite{Rubin1983b}:
\vspace{-0.1cm}
\begin{description}
\item[Strong Ignorability:] Forall $x$ the following hold: \newline
  (1) Unconfoundedness $({Y(0), Y(1) \bigCI T} | X=x)$ and \newline
  (2) Overlap $0 < \textrm{Pr}(T = 1 | X=x) < 1$
\end{description}

The first part, {\em unconfoundedness}, means that, if we partition
the data by the values of the covariate attributes $X=x$, then, within
each group, the treatment assignment and the potential outcomes are independent; then we can
estimate $\ate$ by computing Eq. \ref{eq:indeass} for each value of
the covariates $X=x$ (i.e., conditioning on $X$) and averaging.  The
second part, {\em overlap} is needed to ensure that the conditional
expectations $\E[Y(1)|T=1,X=x]$ and $\E[Y(0)|T=0,X=x]$ are well
defined.  For an illustration of unconfoundedness, suppose we restrict
to flights on dates with similar weather and traffic conditions, by a
fixed carrier, from a fixed airport, etc; then it is reasonable to
assume that $T$ and $Y(0)$ are independent and so are $T$ and $Y(1)$.
In order to satisfy unconfoundedness one has to collect sufficiently
many confounding attributes in $X$ about the data in order to break
any indirect correlations between the treatment and the outcome.

% Notice the need to weight by the
% size of the group, $N_x/N$: some matching techniques discussed below
% ensure that all groups have the same size, and the formula simplifies
% to a standard average.

However, once we include sufficiently many covariate attributes $X$
(as we should) then the data becomes sparse, and many groups $X=x$ are
either empty or have a very small number of items.  For example if a
group has only treated units, then the overlap condition fails, in
other words the conditional expectation $\E[Y(0)|X=x,T=0]$ is
undefined.
% The problem with this approach is that in general the data is
% very sparse: most values of $X=x$ have only one item (thus violating the
% overlap condition) or have only very few items (thus causing the
% empirical estimate of $\E[-]$ to have low precision).
\ignore{In our example, if the covariate attributes include OriginAirportID,
UniqueCarrier,Tempm ,Wspdm and Precipm, because in that case all units
within a group will have the same value of Thunder.}  In general the
strong ignorability assumption is not sufficient to estimate $\ate$ on
observational data.
\vspace{-0.3cm}
\paragraph*{Perfect Balancing}
The solution adopted in statistics is to increase the size of the
groups, while ensuring that strong ignorability holds {\em within each
  group}.  In other words, instead of grouping by the values of the
covariates $X$, one groups by the values of some function on the
covariates $B(X)$.  We say that the groups are strongly ignorable if
the strong ignorability condition holds within each group: \vspace{-0.1cm}
\begin{description}
\item[Strong Ignorability in Groups:] Forall $b$ the following holds:
%
  \newline (1) Unconfoundedness $({Y(0), Y(1) \bigCI T} | B(X)=b)$ and
%
  \newline (2) Overlap $0 < \textrm{Pr}(T = 1 | B(X)=b) < 1$
\end{description}
Rosenbaum and Rubin~\cite{Rubin1983b} gave an elegant characterization
of the functions $B$ that define strongly ignorable groups, which we
review here.  We say that the groups are {\em perfectly balanced}, or
that $B$ is a {\em balancing score} if $X$ and $T$ are independent in
each group, \ie $({X \bigCI T} | B(X)=b)$ for all values $b$.
Equivalently: \vspace{-0.2cm}
\begin{description}
\item[Perfect Balanced Groups:] Within each group $b$, the
  distribution of the covariate attributes of the treated units is the
  same as the distribution of the control units:

\end{description}
 \vspace{-0.3cm}{\small
 \begin{align}
 \forall x:  & \textrm{Pr}(X = x | T = 1, B(X)=b) =
         \textrm{Pr}(X = x | T = 0,B(X)=b) \hfill \label{eq:bs}
  \end{align}}
\vspace{-0.7cm} \begin{theorem}~\cite[Th.3]{Rubin1983b} \label{the:sib} If the
  treatment assignment is strongly ignorable, and $B$ defines
  perfectly balanced groups, then the treatment assignment is strongly
  ignorable within each group.
\end{theorem}
%\vspace{-0.2cm}
\vspace{-0.3cm}
\begin{proof}
  The overlap part of the theorem is trivial, we show
  unconfoundendess.  Abbreviating $Y = (Y(0),Y(1))$, we need to prove:
  if (a) $({Y \bigCI T} | X=x)$ and (b) $({X \bigCI T} | B(X)=b)$,
  then\footnote{This is precisely the Decomposition Axiom in
    graphoids~\cite[the. 1]{PearlBook1998}; see ~\cite{DBLP:journals/ipl/GyssensNG14} for a
    discussion.}  $({Y \bigCI T} | B(X)=b)$.  (a) implies
  $\E[T | Y=y, X=x] = \E[T | X=x]$ and also
  $\E[T | Y=y, X=x, B=b] = \E[T | X=x, B=b]$ since $B$ is a function
  of $X$; (b) implies $\E[T | X=x] = \E[T | X=x, B=b] = \E[T | B=b]$.
  Therefore,
  $\E[T | Y=y, B=b] = \E_x[\E[T | Y=y,X=x,B=b]] = \E_x[\E[T | X=x,
  B=b]] = \E[T | B=b]$ proving the theorem.
\end{proof}


If the treatment assignment is strongly ignorable within each group,
then we can compute $\ate$ by computing the expectations of
Eq.~\ref{eq:ate} in each group, then taking the average (weighted by
the group probability):

\begin{eqnarray}
% \nonumber % Remove numbering (before each equation)
\lefteqn{\ate = \E[Y(1)-Y(0)] = \E_b[\E[Y(1)-Y(0)| B(X)=b]]} \label{eq:oatt} \\
  &=& \E_b[\E[Y(1)|B(X)=b]]-\E_b[\E[Y(0)|B(X)=b]] \nonumber \\
  &=& \E_b[\E[Y(1)|T=1,B(X)=b]]-\E_b[\E[Y(0)|T=0,B(X)=b]] \nonumber
\end{eqnarray}



Thus, one approach to compute causal effect in observational data is
to use group the items into balanced groups, using a balancing fuction
$B$.  Then, $\ate$ can be estimated using the formula above, which can
be translated into a straightforward SQL query using simple
selections, group-by, and aggregates over the relation $R$.  This
method is called {\em subclassification} in statistics.  The main
problem in subclassification is finding a good balancing score $B$.
Rosenbaum and Rubin proved that the best balancing score is the
function $E(x) = \textrm{Pr}(T = 1 | X=x)$, called {\em propensity
  score}.  However, in practice the propensity score $E$ is not
available directly, instead needs to be learned from the data using
logistic regression, and this leads to several
problems~\cite{king15}.  When no good balancing function can be found,
a related method is used, called matching.


\vspace{-.25cm}

\paragraph*{Matching} We briefly describe matching
following~\cite{Rubin1983b}.  Consider some balancing score $B(X)$
(for example the propensity score).  One way to estimate the quantity
in Eq.~\ref{eq:oatt} is as follows.  First randomly sample the value
$b$, then sample one treated unit, and one control unit with $B(X)=b$.
This results in a set of treated units $i_1, i_2, \ldots, i_m$ and a
matching set of control units $j_1, j_2, \ldots, j_m$: then the
difference of their average outcome
$\sum_k (Y_{i_k}(1) - Y_{j_k}(0))/m$ is an unbiased estimator of
$\E_b[\E[Y(1)-Y(0)| B(X)=b]]$ and, hence, of $\ate$.  \ignore{Notice that
there is no need to weight by the group size, because the ratio of
treated/untreated units is the same in each group.}  Generally, the
matching technique computes a subset of units consisting of all
treated units and, for each treated unit, a randomly chosen sample of
fixed size of control units with the same value of balancing score.

% The motivation for this technique is to reduce the cost associated
% with measuring outcome. For example, in clinical studies typically a
% few patients that are treated with a drug exists that can be matched
% with potentially a very large number of control units. Measuring an
% outcome for all possible control units might become infeasible. Notice
% that matching or grouping based on a balancing score can be performed
% before measuring an outcome (see \cite{Rubin1983b} and references
% therein).

Historically, matching  predated
subclassification, and can be done even when no good balancing score
is available.
%
% when no good
% Ideally, treated and control units would exactly matched on some
% balancing score $B(X)$, so that the {\em sample} distribution of
% covariates $X$ would be identical in two groups. In practice, however,
% exact matches and thus groups even on a scalar balancing score are
% often impossible to obtain so methods which seeks approximate matches
% must be used \cite{Rubin1983b}.
%
The idea is to match each treated unit with one or multiple control
units with ``close'' values of the covariate attributes $X$, where
closeness is defined using some distance function $\delta(x_i,x_j)$
between the covariate values of two units $i$ and $j$.  The most
commonly used distance functions are listed in
Fig.~\ref{fig:metrics}. \footnote{The distance functions in Table
  \ref{fig:metrics} are {\em semi or pseudo-metrics}. That is they are
  symmetric; they satisfy triangle inequality and $x_i=x_j$ implies
  $\delta(x_i,x_j)=0$, but the converse does not hold.} The efficacy
of a matching method is evaluated by measuring {\em degree of
  imbalance} i.e., the differences between the distribution of
covariates in two groups in the matched subset. Since there is no
generic metric to compare two distributions, measures such as mean,
skewness, quantile and multivariate histogram are used for this
purpose.  A rule of thumb is to
evaluate different distance metrics and matching methods until a
well-balance matched subset with a reasonable size obtained.



\ignore{
\paragraph*{Matching}
Instead of grouping by some balancing score $B(x)$, several techniques
in the statistics literature compute the groups using {\em
  matching}~\cite{Rubin1974}, which computes a subset of units
consisting of all treated units and, for each treated unit, a random
sample of fixed size of control units with ``close'' values of the
covariate attributes, where closeness is defined using one of the
distance measures $\delta(i,j)$ between units listed in
Fig.~\ref{fig:metrics}.  To see the intuition behind matching, suppose
the data is dense, and we can match each treated unit with one control
unit with the same values of the covariate attributes.  Then, each
group (defined by the value of the covariates) is perfectly balanced,
and moreover the ratio of treated/control units is the same in all
groups, hence we do not care about the group sizes in
Eq.\ref{eq:oatt}.  In general, matching relaxes this principle, by
matching each treated unit with a fixed number of control units (still
ensuring a constant ratio) and relaxing the equality condition to
closeness according to some measure.  Thus, the main computational
challenge in causal inference consists of matching: we describe it in
detail in Sec.~\ref{?????}.  \dan{I don't like how this paragraph come out}}



\vspace{-.3cm}
\paragraph*{Summary}
The goal of causal analysis is to compute $\ate$ (Eq.~\ref{eq:ate})
and the main challenge is that each record misses one of the outcomes,
$Y(1)$ or $Y(0)$.  A precondition to overcome is to ensure strong
ignorability, by collecting sufficiently many covariate attributes
$X$.  For the modern data analyst this often means integrating the
data with many other data sources, to have as much information
available as possible about each unit.  One caveat is that one should
not include attributes that are themselves affected by the treatment;
the principled method for choosing the covariates is based on
graphical models~\cite{de2011covariate}.  Once the data is properly
prepared, the main challenge in causal analysis \ignore{ \footnote{There are
  some model-based alternatives to matching e.g., covariates
  adjustment on random samples.  However, matching have several nice
  properties that makes it more appealing in practice (see,
  \cite{Rubin1983b}).}} is {\em matching} data records such as to
ensure that the distribution of the covariates attributes of the
treated and untreated units are as close as possible
(Eq.(\ref{eq:bs})).  This will be the focus of the rest of our paper.
Once matching is performed, $\ate$ can be computed using
Eq.(\ref{eq:oatt}).  Thus, the main computational challenge in causal
analysis is the matching phase, and this paper describes scalable
techniques for performing matching in a relational database system.

% \dan{the old subsection with distance measures is commented in the
%   Latex, and replaced by the figure.  Still need to add citations, and
%   to discuss Coarsening Exact Distance (the problem is that it is
%   technically not a distance).}
%
\ignore{
\subsection{Distance Measures in Matching}
\label{sec:matching}


Matching is a non-parametric pre-processing step that identifies
data subsets from which causal inference can be drawn with reduced bias and model-dependence \cite{ho2005}.
In this approach, the goal is to match every treated units with one (or more) control unit(s)  with similar values of covariates. Thus,
the first step  is to choose a measure  of similarity or closeness between the
units, i.e., a metric that determines whether an individual is a good match for another. In an ideal world, for each treated unit, there exists a control unit with exactly the same values of covariates. Then,  the {\em exact distance}, defined below, can be used for matching.

\vspace{.2cm}
\noindent  {\bf Exact Distance:}  \[ \delta(i,j) =
  \begin{cases}
    0       & \quad \text{if } X_i=X_j\\
    	\infty  & \quad \text{if } X_i \not =X_j\\

  \end{cases}
\]

\noindent This approach fails in finite samples if the dimensionality of $X$ is large;
it is simply impossible if $X$ contains continuous covariates. In general, alternative
distance measures must be used.\\

\ignore{
Next we briefly overview some  techniques that are representative of the large
variety used in the literature based on \cite{Rubin1983b,Stuart10,Sekhon08,Shalizi13,king15}.}

\ignore{
Matching methods consist of the following steps: \ (1) choosing a methods of similarity or closeness between the
 units i.e., a metric that determine weather an individual is a good match for
  another; \ (2) devising an strategy to match units given a
  measure of closeness; \ (3) checking the balance of the matched data and
  iterating steps 1 and 2 until a well-matched sample obtained.}

\ignore{
The ultimate goal of the matching technies goal then is to maximize both balance, the similarity between the multivariate
distributions of the treated and control units, and the size of the matched data set.
Any remaining imbalance must be dealt with by statistical modeling assumptions. The
primary advantage of matching is that it greatly reduces the dependence of our conclusions
on these assumptions (Ho et al., 2007).}


\ignore{
Matching is a nonparametric method of controlling for the confounding influence of pretreatment control variables in observational data. The key goal of matching is to prune observations from the data so that the remaining data have better balance between the treated and control groups, meaning that the empirical distributions of the covariates (X) in the groups are more similar. Exactly balanced data mean that controlling further for X is unnecessary (since it is unrelated to the treatment variable), and so a simple difference in means on the matched data can estimate the causal effect

Matching is a statistical technique which is used to evaluate the effect of a treatment by comparing the treated and the non-treated units in an observational study or quasi-experiment (i.e. when the treatment is not randomly assigned). The goal of matching is, for every treated unit, to find one (or more) non-treated unit(s) with similar observable characteristics against whom the effect of the treatment can be assessed. By matching treated units to similar non-treated units, matching enables a comparison of outcomes among treated and non-treated units to estimate the effect of the treatment reducing bias due to confounding


As is widely recognized, matching methods for causal inference are best applied with
an extensive, iterative, and typically manual search across different matching solutions,
simultaneously seeking to maximize covariate balance between the treated and control
groups and the matched sample size.




To evaluate a matching method, we confront the same bias-variance trade-off as exists
with most statistical methods. Thus, instead of bias, we focus on reducing the closely related quantity, imbalance, the difference between the multivariate empirical densities of the treated and control units (for the specific mathematical relationship between the two, see Imai, King and Stuart, 2008). Similarly, the variance of the causal effect estimator can be reduced when heterogeneous observations are pruned by matching, but a too small matched sample size can inflate the variance

Thus, in matching, the bias-variance trade off is affected through the crucial trade off between the degree of imbalance and the size of the matched sample. }



\ignore{
\noindent  {\bf Exact Distance:}  \[ \delta(i,j) =
  \begin{cases}
    0       & \quad \text{if } X_i=X_j\\
    	\infty  & \quad \text{if } X_i \not =X_j\\

  \end{cases}
\]
}
 \ignore{In this approach each treated unit is matched to all
possible control units with exactly the same values on all the covariates, forming subclasses
such that within each subclass all units (treatment and control) have the same covariate values.  This approach fails in finite samples if the dimensionality of $X$ is large
and is simply impossible if $X$ contains continuous covariates. Thus, in general, alternative
methods must be used.\\}


\ignore{
If your controls are all normally distributed (more precisely, follow
and elliptic distribution) and your sample size is large enough, then
matching on Mahalanobis distance has the Equal Percent Bias
Reduction (EPBR) property}

\noindent  {\bf Propensity Score (PS):}

$$\delta(i,j) =|e(x_i)-e(x_j)|$$

\noindent where $ e(x_i)=\textrm{Pr}(T_i=1|X=x_i)$, the {\em propensity score},  is the probability of a
 unit (e.g., person, classroom, school) being assigned to a particular treatment
 given a set of observed covariates. It is shown that that propensity score is a {\em balancing score} \cite{Rubin1983b}.
  A balancing score, $b(x_i)$, is a function of an observed covariates such
  that the distribution of $x$ given $b(x_i)$ is the same for treated and
  control units. It has been shown that if
  treatment assignment is strongly ignorable given $x_i$, then it is strongly
   ignorable given any balancing score \cite{Rubin1983b}. Therefore, instead of conditioning on $x_i$, we can condition on the balancing score and obtain an unbiased
     estimate of the average treatment effect at a particular value of balancing score.   Notice that when we have a sample from a population, the propensity score is unknown. In practice, it is almost always estimated by assuming a logistic regression model.

\ignore{
The distances described above can be combined meaning that exact
matching can be done on key covariates such as carrier or origin and
 within each group matching based on PS can be applied. If key covariates of interest
 are continues MD matching can be applied with propensity score calipers. Both of
  these combined techniques would yield well balance on the key
  covariates and relatively well balance on the rest.}

\vspace{0.2cm}
\noindent  {\bf Mahalanobis Distance (MD):}
 $$\delta(i,j) =(x_i-x_j)'\Sigma^{-1} (x_i-x_j)$$

\noindent   MD generalize Euclidian distance. It occurs when the correlation in the data is also taken into account using the inverse of the variance-covariance matrix $\Sigma$.


\ignore{\paragraph*{Matching methods:}}




\vspace{.2cm}
\noindent  {\bf Coarsen Exact Distance:} This metric has been recently proposed  in \cite{IacKinPor09}. Its central idea is to perform exact matching on broader ranges of  variables. Specifically, each covariate is temporally coarsened
according to a pre-specified set of cutpoints. These cutpoints are  chosen either by the analyst or
computed using automatic discretization methods. \ignore{Coarsening or bining is widely used in data analysis. However,
unlike the general use of bining in data analysis which involved with permanent removal of information, in this
approach, coarsening is done only for the sake of matching and the analysis is done with the
actual data.} \ignore{For example, in the context of weather forecast, the following coarsening of barometric pressure of corresponds categories that provide a reasonable weather forecast \cite{barometricpressureheadache:article}:

 \vspace{0.3cm}
  \hspace{-0.4cm}
 {\tiny
 \begin{tabular}{|c|c|c|}
   \hline
   % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
   Range& Bucket & Forecast \\
   \hline
   Over 1022.69 mbar & High  & Cloudy, Warmer \\
   Between 1022.69 mbar  and  1009.14 mbar& Mediate & Precipitation likely  \\
   Under  1009.14 mbar  & Low & Storm \\
   \hline
 \end{tabular}
}




 \vspace{0.3cm}}




Once a distance measure is chosen, the next step is to choose a method to perform the actual matching.  These methods are discussed in detail in Section \ref{sec:BasicTechniques}. The final step is to evaluate whether a matching method was successful, which we discuss in the next section.


\subsection{Evaluating Matching Techniques}

As we discussed before, matching aims to prune the input data such that what remains
has {\em balance} between the treated and control groups, meaning that the empirical distributions
of the covariates in the groups are more similar. There is no general agreement on the metric to choose for comparing two distributions. In practice, metrics such as mean, skewness, quantile and  multivariate histogram of the distribution of covarites in  treated and control group are used to measure the degree of imbalance.

To evaluate a matching method, we confront the same bias-variance trade-off as exists
with most statistical methods \cite{king15}.  The variance is related to  the number of matched units obtained, while  the bias is related to the degree of imbalance. \ignore{, i.e., the difference between the distribution of covariates between  treated and control units in the matched sample. For example, exact matching completely removes the bias by ensuring a perfect balance. However, in practice, zero or few units might be obtained by applying this method, which makes the estimation of average treatment effect either impossible or sensitive to sampling variance. } In particular, the bias of a causal effect estimator can be reduced by matching, but a matched sample that is too small can inflate the variance.
A rule of thumb is to evaluate different metrics and matching strategies until a well-balance matched with reasonable size obtained.

\ignore{Thus, in matching, we have a bias-variance trade off through the crucial trade off between the degree of imbalance and the size of the matched sample.}
\ignore{
Propensity score:  Matching methods based on propensity score is by far the prominent approach in practice. However, using propensity score for matching has been object to some criticisms lately \cite{king15}. It has been shown that propensity score matching approximate a randomize experiment i.e., the balance is only granted across the spectrum of all possible samples. Whereas, other matching methods, we mentioned in this paper, approximate a fully blocked experiment \cite{king15} (see Section \ref{sec:NRCM}). In observational setting since we have only one sample at hand, other matching methods would dominate propensity score matching.

Mahalanobis distance: It is known that this metric may exhibit some odd behavior when: the covariates
are not normally distributed;  there are relatively large number of covariates; there are extremely outlying
observations; when there are dichotomous variables \cite{rosenbaum2009}. \ignore{\em This measure is only mentioned for the
sake of completeness and will not be used in the subsequent discussions of this paper.}

Coarsening:
Several nice properties of matching by coarsening has been established in \cite{IacKinPor09}. For instance, unlike other
 approaches, the causal effect estimation error and the imbalance is bound by the user, therefor
  the labourers process of matching and checking for balance is not needed anymore. More
   importantly, this approach meets the {\em congruous principle} according
   which there should be a congruous between analysis space
   and data space. Notice that MD and PS project a vector from
    multidimensional space into a scalar value. It has been argued that methods violating the congruous principle
    may to lead to less robust inference with suboptimal and highly counterintuitive
     properties \cite{IacKinPor09}. Our experience with the flight and weather
     dampest confirms the mentioned theories.


\vspace{.2cm}


\ignore{\noindent {\bf Nearest Neighbor and optimal Matching:} The most common matching method is $k:1$ nearest
neighbor matching \cite{Rubin1974}. This method selects the $k$  best control matches for each individual
in the treatment group. Matches
are chosen for each treated unit one at a time, with a pre-specified ordering such
as largest to smallest. At each matching step we choose the control unit
that is not yet matched but is closest to the treated unit on the
distance measure. In its simplest form, 1:1 nearest neighbor matching selects for each treated individual $i$ the
control individual with the smallest distance from individual $i$.

In nearest neighbor matching, closest control match for each treated unit is chosen one at a time, without trying to minimize
a global distance measure. In addition the order in which the treated units are matched may change
the quality of the matching.  {\em Optimal matching} circumvents these issues by taking into account the overall set of
matches when choosing individual matches, minimizing a global distance measure
\cite{rosenbaum2002observational}. Greedy matching performs poorly when there is intense
competition for controls, and performs well when there is little competition \cite{Rosenbaum93}. However, it has been shown that optimal matching does not in general
perform any better than greedy matching in terms of creating {\em groups} with good balance \cite{Rosenbaum93}.

Notice that in the mentioned  methods,  matching is done without replacement. When there are a few control individuals comparable to the treated individuals matching can be done with replacement. While this would decrease the bias, it makes the analysis more complicated. In this work we only consider matching without replacement.


\vspace{.2cm}
\noindent { \bf Subclassification and full matching:}
 It is easy to see that nearest neighbor matching does not necessarily use all the data, meaning that many control units even though in the range of a treatment unit will be discarded. In subclassification, the idea is to form subclasses, such that in each the distribution  of covariates for the treated and control groups are as similar as possible. In practice usually 5-10 subclasses are used. However, with larger sample sizes more subclasses (e.g., 10-20) may be feasible and appropriate \cite{Lunceford04}. In a particular form of subclassification known as {\em full matching}, the number of subclasses are chosen optimally \cite{rosenbaum2002observational}.


\vspace{.2cm}

\noindent { \bf Objections to the described methods:} In practice, propensity score matching is
the most commonly used matching method  \cite{king15}. It is wildly recognized that, matching methods
based on propensity score (and in general) are best applied with extensive, iterative and typically
manual search across different matching techniques with the goal of maximizing covariates
 balance between treated and control group and the matched sample size. However, the focus
 of the most of these methods is to maximize the size of the matched
 data as oppose to the covariates balance \cite{king15}. The balance is only checked after the fact
 and this process should be iterated until a well-balanced matched is obtained.

As a matter of fact, propensity score matching approximate a
randomize experiment. Whereas other matching methods approximate a fully blocked experiment and dominate propensity score
matching \cite{king15}. Although, among these methods is has lees applicability. In fact,
it is known that it may exhibit some odd behavior when the covariates
are not normally distributed or the are relatively large number of covariates.

\vspace{.2cm}
\noindent { \bf Coarsen Exact Matching (CEM):} The central idea in CEM is to do exact matching on
broader ranges of the variables. Specifically, each covariates is temporally coarsened
according to a prespecified set of cutpoints. These cutpoints are either chosen by the analysis or
computed using automated histogram methods. Coarsening or bining is widely used in data analysis. However,
unlike the general use of bining in data analysis which involved with permanent removal of information, in this
approach, coarsening is done only for the sake of matching and the analysis is done with the
 actual data.   Next, the units are grouped into strata, each of which has the same values of the
 coarsened covariates. Finally, any stratum that does not contain at least one treated and
 one control unit will be pruned from data \cite{IacKinPor09}.

Several nice properties of CEM has been established in \cite{IacKinPor09}. For instance, unlike other
 approaches, the causal effect estimation error and the imbalance is bound by the user, therefor
  the labourers process of matching and checking for balance is not needed anymore. More
   importantly, CEM meets the {\em congruous principle} according
   which there should be a congruous between analysis space
   and data space. Notice that MD and PS project a vector from
    multidimensional space into a scalar value. It has been argued that methods violating the congruous principle
    may to lead to less robust inference with suboptimal and highly counterintuitive
     properties \cite{IacKinPor09}. Our experience with the flight and weather
     dampest confirms the mentioned theories. Therefore, in this work we mainly focus on the CEM method.

\ignore{
 only approximate a random experiment while other methods approximate
fully blocked experiment. It is known that fully blocked experiment dominate randomize experiment. This is simply because randomize experiment grantee covariates balance on average taken over the sampling distribution, whereas fully blocked experiment grantees covariates balance on a single sampled. Therefore, other matching methods PSM in observation setting in which only one sample is at hand.

While Mahalanobis distance matching would approximate a fully blocked experiment, }










\ignore{

\paragraph*{Nearest Neighbor Matching:}
This method selects the r (default=1) best control matches for each individual
in the treatment group (excluding those discarded using the discard option). Matching is
done using a distance measure specified by the distance option (default=logit). Matches
are chosen for each treated unit one at a time, with the order specified by the m.order
command (default=largest to smallest). At each matching step we choose the control unit
that is not yet matched but is closest to the treated unit on the distance measure.


\paragraph*{Coarsened Exact Matching:} This method is a Monotonoic Imbalance Bounding (MIB) matching
method â€” which means that the balance between the treated and control groups is chosen by
the user ex ante rather than discovered through the usual laborious process of checking after
the fact and repeatedly reestimating, and so that adjusting the imbalance on one variable has
no effect on the maximum imbalance of any other.



\paragraph*{Subclassification:}

}

}


}
} 